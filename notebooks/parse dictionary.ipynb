{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a.', 'a-', 'a 1', 'aam', 'aard-vark', 'aard-wolf', 'aaronic; aaronical', \"aaron's rod\", 'ab-', 'ab', 'abaca', 'abacinate', 'abacination', 'abaciscus', 'abacist', 'aback', 'abactinal', 'abaction', 'abactor', 'abaculus', 'abacus', 'abada', 'abaddon', 'abaft', 'abaisance', 'abaiser', 'abaist', 'abalienate', 'abalienation', 'abalone', 'aband', 'abandon', 'abandoned', 'abandonedly', 'abandonee', 'abandoner', 'abandonment', 'abandum', 'abanet', 'abanga', 'abannation; abannition', 'abarticulation', 'abase', 'abased', 'abasedly', 'abasement', 'abaser', 'abash', 'abashedly', 'abashment', 'abasia', 'abassi; abassis', 'abatable', 'abate', 'abatement', 'abater', 'abatis; abattis', 'abatised', 'abator', 'abattoir', 'abature', 'abatvoix', 'abawed', 'abaxial; abaxile', 'abay', 'abb', 'abba', 'abbacy', 'abbatial', 'abbatical', 'abbe', 'abbess', 'abbey', 'abbot', 'abbotship', 'abbreviate', 'abbreviated', 'abbreviation', 'abbreviator', 'abbreviatory', 'abbreviature', 'abb wool', 'a b c', 'a b c\".', 'abdal', 'abderian', 'abderite', 'abdest', 'abdicable', 'abdicant', 'abdicate', 'abdication', 'abdicative', 'abdicator', 'abditive', 'abditory', 'abdomen', 'abdominal', 'abdominales']\n"
     ]
    }
   ],
   "source": [
    "# Note: Webster's Unabridged Dictionary from https://www.gutenberg.org/ebooks/29765\n",
    "\n",
    "f = open(\"gutenberg_dict.txt\", \"r\")\n",
    "\n",
    "lines = f.readlines()\n",
    "\n",
    "f.close()\n",
    "\n",
    "words = []\n",
    "\n",
    "for line in lines:\n",
    "    if line.isupper():\n",
    "        #print(line)\n",
    "        new_word = line.lower().strip()\n",
    "        if new_word not in words:\n",
    "            words.append(new_word)\n",
    "        \n",
    "print(words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99041\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create numpy array to store word vectors\n",
    "\n",
    "#currently set to store top 1000 words as their 100 dimensional word vectors\n",
    "\n",
    "arr = np.ndarray(shape=(99041, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load fasttext model\n",
    "\n",
    "model = fasttext.load_model(\"/Users/alex/result/cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store word vectors for the 1000 words in numpy array\n",
    "\n",
    "for i in range(0, 99041):\n",
    "    arr[i] = model.get_word_vector(words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity of word vectors\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# cos_sim = dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-70-e0564a75c14a>:10: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  current = dot(user_word_vec, arr[i]) / (norm(user_word_vec) * norm(arr[i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanative 0.6114206927373194\n",
      "(0.6662489771842957, 'discriptive')\n"
     ]
    }
   ],
   "source": [
    "# user word\n",
    "user_word = \"descriptive\"\n",
    "\n",
    "user_word_vec = model.get_word_vector(user_word)\n",
    "\n",
    "# iterate through numpy array and find vector with greatest cosine similarity\n",
    "index = -1\n",
    "cos_sim = -1\n",
    "for i in range(0, 99041):\n",
    "    current = dot(user_word_vec, arr[i]) / (norm(user_word_vec) * norm(arr[i]))\n",
    "    if current > cos_sim and words[i] != user_word and user_word[0:2] != words[i][0:2]:\n",
    "        cos_sim = current\n",
    "        index = i\n",
    "\n",
    "print(words[index] + \" \" + str(cos_sim))\n",
    "\n",
    "print(model.get_nearest_neighbors(user_word)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"best\" in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36466"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.index(\"giggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36466\n"
     ]
    }
   ],
   "source": [
    "i = words.index(\"giggle\")\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8299897601560428\n"
     ]
    }
   ],
   "source": [
    "print(dot(user_word_vec, arr[i]) / (norm(user_word_vec) * norm(arr[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'following', 'day']\n"
     ]
    }
   ],
   "source": [
    "# user sentence\n",
    "user_sent = \"The following day\"\n",
    "\n",
    "user_sent = user_sent.lower()\n",
    "\n",
    "user_sent_words = user_sent.split()\n",
    "\n",
    "print(user_sent_words)\n",
    "\n",
    "vec1 = model.get_word_vector(user_sent_words[0])\n",
    "\n",
    "vec2 = model.get_word_vector(user_sent_words[1])\n",
    "\n",
    "vec3 = model.get_word_vector(user_sent_words[2])\n",
    "\n",
    "vec4 = []\n",
    "for i in range(0, 300):\n",
    "    vec4.append((vec1[i] + vec2[i] + vec3[i]) / 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-86-870950781c36>:7: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  current = dot(user_word_vec, arr[i]) / (norm(user_word_vec) * norm(arr[i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0.6493477675471379\n"
     ]
    }
   ],
   "source": [
    "user_word_vec = vec4\n",
    "\n",
    "# iterate through numpy array and find vector with greatest cosine similarity\n",
    "index = -1\n",
    "cos_sim = -1\n",
    "for i in range(0, 99041):\n",
    "    current = dot(user_word_vec, arr[i]) / (norm(user_word_vec) * norm(arr[i]))\n",
    "    if current > cos_sim and words[i] not in user_sent_words:\n",
    "        cos_sim = current\n",
    "        index = i\n",
    "\n",
    "print(words[index] + \" \" + str(cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
